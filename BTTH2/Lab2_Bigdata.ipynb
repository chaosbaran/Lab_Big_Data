{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_1",
        "outputId": "cdf33f47-8267-4069-b232-01b5f103d880"
      },
      "source": [
        "!pip install findspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RnRE1vVN9C4",
        "outputId": "dc78d984-298f-44f2-8907-e46dacf13823"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tzMJogrVtx0",
        "outputId": "9b950008-9839-4c04-feb9-1a8b2c2c0394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sunset Boulevard (1950) AverageRating: 4.36 (TotalRatings: 7)\n",
            "The Lord of the Rings: The Fellowship of the Ring (2001) AverageRating: 3.89 (TotalRatings: 18)\n",
            "No Country for Old Men (2007) AverageRating: 3.89 (TotalRatings: 18)\n",
            "Mad Max: Fury Road (2015) AverageRating: 3.47 (TotalRatings: 18)\n",
            "Fight Club (1999) AverageRating: 3.50 (TotalRatings: 7)\n",
            "Lawrence of Arabia (1962) AverageRating: 3.44 (TotalRatings: 18)\n",
            "The Godfather: Part II (1974) AverageRating: 4.00 (TotalRatings: 17)\n",
            "The Social Network (2010) AverageRating: 3.86 (TotalRatings: 7)\n",
            "The Terminator (1984) AverageRating: 4.06 (TotalRatings: 18)\n",
            "The Silence of the Lambs (1991) AverageRating: 3.14 (TotalRatings: 7)\n",
            "Gladiator (2000) AverageRating: 3.61 (TotalRatings: 18)\n",
            "Psycho (1960) AverageRating: 4.00 (TotalRatings: 2)\n",
            "E.T. the Extra-Terrestrial (1982) AverageRating: 3.67 (TotalRatings: 18)\n",
            "The Lord of the Rings: The Return of the King (2003) AverageRating: 3.82 (TotalRatings: 11)\n",
            "\n",
            "Sunset Boulevard (1950) is the highest rated movie with an average rating of 4.36 among movies with at least 5 ratings.\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def bai1():\n",
        "    #getOrCreate() sẽ lấy context hiện có hoặc tạo mới nếu chưa có\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    #Load Data cả 2 file ratings và gộp chúng lại (union)\n",
        "    ratings_rdd_1 = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_1.txt\")\n",
        "    ratings_rdd_2 = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_2.txt\")\n",
        "    all_ratings_rdd = ratings_rdd_1.union(ratings_rdd_2)\n",
        "\n",
        "    #Load file movies\n",
        "    movies_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/movies.txt\")\n",
        "\n",
        "    #(UserID, MovieID, Rating, Timestamp) -> (MovieID, (Rating, 1))\n",
        "    def parse_rating(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            movie_id = parts[1] # MovieID o vi tri 1\n",
        "            rating = float(parts[2]) # Rating o vi tri 2\n",
        "            return (movie_id, (rating, 1)) # (Key, (Gia tri, Bo dem))\n",
        "        except:\n",
        "            return (\"ERROR\", (0, 0)) # Bo qua neu dong bi loi\n",
        "\n",
        "    ratings_parsed = all_ratings_rdd.map(parse_rating).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    #Tính tổng rating (sum) và tổng số lượt đếm (count) cho mỗi MovieID\n",
        "    #reduceByKey se cong 2 tuple voi nhau: (a[0] + b[0], a[1] + b[1])\n",
        "    #Ket qua: (MovieID, (TotalRatingSum, TotalRatingCount))\n",
        "    ratings_sum = ratings_parsed.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "    #Tính rating trung binh\n",
        "    #(MovieID, (AverageRating, TotalRatingCount))\n",
        "    ratings_avg = ratings_sum.mapValues(lambda total: (total[0] / total[1], total[1]))\n",
        "\n",
        "\n",
        "    #(MovieID, Title, Genres) -> (MovieID, Title)\n",
        "    def parse_movie(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            movie_id = parts[0]\n",
        "            title = parts[1]\n",
        "            return (movie_id, title)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    movies_parsed = movies_rdd.map(parse_movie).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    #(MovieID, Title) join (MovieID, (AverageRating, TotalRatingCount))\n",
        "    #Ket qua: (MovieID, (Title, (AverageRating, TotalRatingCount)))\n",
        "    joined_data = movies_parsed.join(ratings_avg)\n",
        "\n",
        "    #Chuyen doi sang dinh dang: \"MovieTitle AverageRating: xx (TotalRatings: xx)\"\n",
        "    def format_output(record):\n",
        "        movie_id, (title, (avg_rating, total_count)) = record\n",
        "        # Dinh dang diem trung binh lay 2 so thap phan\n",
        "        return f\"{title} AverageRating: {avg_rating:.2f} (TotalRatings: {total_count})\"\n",
        "    output_strings = joined_data.map(format_output)\n",
        "\n",
        "    #Thu thap ket qua va in ra\n",
        "    for line in output_strings.collect():\n",
        "        print(line)\n",
        "    print()\n",
        "\n",
        "    #Loc ra cac phim co >= 5 luot danh gia\n",
        "    #record[1] la (Title, (AverageRating, TotalRatingCount))\n",
        "    #record[1][1] la (AverageRating, TotalRatingCount)\n",
        "    #record[1][1][1] la TotalRatingCount\n",
        "    filtered_movies = joined_data.filter(lambda record: record[1][1][1] >= 5)\n",
        "\n",
        "    if filtered_movies.isEmpty():\n",
        "        print(\"Khong tim thay phim nao co tu 5 danh gia tro len.\")\n",
        "    else:\n",
        "        #Tim phim co diem trung binh cao nhat\n",
        "        #Su dung reduce de tim ra record co avg_rating (a[1][1][0]) cao nhat\n",
        "        highest_rated_record = filtered_movies.reduce(\n",
        "            lambda a, b: a if a[1][1][0] > b[1][1][0] else b\n",
        "        )\n",
        "\n",
        "        #Lay thong tin tu record tim duoc\n",
        "        title, (avg_rating, total_count) = highest_rated_record[1]\n",
        "\n",
        "        print(f\"{title} is the highest rated movie with an average rating of {avg_rating:.2f} among movies with at least 5 ratings.\")\n",
        "\n",
        "#Ham chinh de chay Spark\n",
        "if __name__ == \"__main__\":\n",
        "    #Khoi tao SparkSession de lay SparkContext\n",
        "    spark = SparkSession.builder.appName(\"Bai1\").getOrCreate()\n",
        "    bai1()\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 2:"
      ],
      "metadata": {
        "id": "T_Pueo-0Sqxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def bai2():\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    try:\n",
        "        #Load ratings (gop 2 file)\n",
        "        ratings_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_1.txt\").union(sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_2.txt\"))\n",
        "        #Load movies\n",
        "        movies_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/movies.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"LOI: Khong tim thay file. Hay kiem tra duong dan: {e}\")\n",
        "        return\n",
        "\n",
        "    #Parse Movies: (MovieID, Genres)\n",
        "    def parse_movie_genres(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            movie_id = parts[0]\n",
        "            # Lay phan tu cuoi cung lam Genre (tranh loi neu ten phim co dau phay)\n",
        "            genres = parts[-1]\n",
        "            return (movie_id, genres)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    # Parse Ratings: (MovieID, Rating)\n",
        "    def parse_rating(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            movie_id = parts[1]\n",
        "            rating = float(parts[2])\n",
        "            return (movie_id, rating)\n",
        "        except:\n",
        "            return (\"ERROR\", 0.0)\n",
        "\n",
        "    movies_parsed = movies_rdd.map(parse_movie_genres).filter(lambda x: x[0] != \"ERROR\")\n",
        "    ratings_parsed = ratings_rdd.map(parse_rating).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "\n",
        "    # (MovieID, Genres) JOIN (MovieID, Rating) => (MovieID, (Genres, Rating))\n",
        "    joined_data = movies_parsed.join(ratings_parsed)\n",
        "\n",
        "    # Ham nay se nhan 1 dong du lieu cua phim va tra ve NHIEU dong cho tung the loai\n",
        "    def split_genres(record):\n",
        "        movie_id, (genres_str, rating) = record\n",
        "        genre_list = genres_str.split('|') # Tach chuoi bang dau gach dung\n",
        "\n",
        "        results = []\n",
        "        for genre in genre_list:\n",
        "            # Tra ve tuple: (TenTheLoai, (DiemRating, 1_luot_danh_gia))\n",
        "            results.append((genre, (rating, 1)))\n",
        "        return results\n",
        "\n",
        "    # Su dung flatMap de lay danh sach ra\n",
        "    genre_ratings = joined_data.flatMap(split_genres)\n",
        "\n",
        "    # ReduceByKey: Cong don diem va so luot danh gia cho tung the loai\n",
        "    # (Genre, (SumRating, Count))\n",
        "    genre_stats = genre_ratings.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "    # MapValues: Tinh trung binh\n",
        "    # (Genre, (Average, Count))\n",
        "    final_result = genre_stats.mapValues(lambda x: (x[0] / x[1], x[1]))\n",
        "\n",
        "    def format_output(record):\n",
        "        genre, (avg_rating, count) = record\n",
        "        # Format theo yeu cau: \"Genre - AverageRating (TotalRatings)\"\n",
        "        return f\"{genre} - {avg_rating:.2f} ({count})\"\n",
        "    output_lines = final_result.map(format_output)\n",
        "\n",
        "    # Thu thap va in ket qua\n",
        "    for line in output_lines.collect():\n",
        "        print(line)\n",
        "\n",
        "# Chay\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"Bai2\").getOrCreate()\n",
        "    bai2()\n",
        "    spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6GndihbSr1l",
        "outputId": "55af5131-c89e-4f79-ac48-1fba5c1d61c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama - 3.76 (128)\n",
            "Action - 3.71 (54)\n",
            "Sci-Fi - 3.73 (54)\n",
            "Biography - 3.56 (25)\n",
            "Family - 3.67 (18)\n",
            "Horror - 4.00 (2)\n",
            "Fantasy - 3.86 (29)\n",
            "Thriller - 3.70 (27)\n",
            "Mystery - 4.00 (2)\n",
            "Adventure - 3.63 (83)\n",
            "Film-Noir - 4.36 (7)\n",
            "Crime - 3.81 (42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 3:"
      ],
      "metadata": {
        "id": "Z2eLrw4dI9UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def bai3():\n",
        "    # Khoi tao SparkContext\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    try:\n",
        "        # Load ratings (gop 2 file)\n",
        "        ratings_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_1.txt\").union(sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_2.txt\"))\n",
        "        # Load users\n",
        "        users_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/users.txt\")\n",
        "        # Load movies\n",
        "        movies_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/movies.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"LOI: Khong tim thay file. Vui long kiem tra duong dan: {e}\")\n",
        "        return\n",
        "\n",
        "    # Parse Users: (UserID, Gender)\n",
        "    def parse_user(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            return (parts[0], parts[1]) # (UserID, Gender)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    # Parse Ratings: (UserID, (MovieID, Rating))\n",
        "    # Chon UserID lam Key de ti nua join voi bang Users\n",
        "    def parse_rating(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            user_id = parts[0]\n",
        "            movie_id = parts[1]\n",
        "            rating = float(parts[2])\n",
        "            return (user_id, (movie_id, rating)) # (UserID, (MovieID, Rating))\n",
        "        except:\n",
        "            return (\"ERROR\", (0, 0))\n",
        "\n",
        "    users_parsed = users_rdd.map(parse_user).filter(lambda x: x[0] != \"ERROR\")\n",
        "    ratings_parsed = ratings_rdd.map(parse_rating).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    # (UserID, (MovieID, Rating)) JOIN (UserID, Gender) => (UserID, ((MovieID, Rating), Gender))\n",
        "    joined_user_rating = ratings_parsed.join(users_parsed)\n",
        "\n",
        "    # Chuyen doi sang dang: (MovieID, (Rating, Gender))\n",
        "    def map_to_movie_gender(record):\n",
        "        # record: (UserID, ((MovieID, Rating), Gender))\n",
        "        user_id, ((movie_id, rating), gender) = record\n",
        "        return (movie_id, (rating, gender))\n",
        "    movie_gender_rdd = joined_user_rating.map(map_to_movie_gender)\n",
        "\n",
        "    # Map sang dang vector de cong don:\n",
        "    # (MovieID, (SumMale, CountMale, SumFemale, CountFemale))\n",
        "    def map_gender_stats(record):\n",
        "        movie_id, (rating, gender) = record\n",
        "        if gender == 'M':\n",
        "            return (movie_id, (rating, 1, 0, 0)) # Diem Nam, 1 luot Nam, 0 Nu\n",
        "        elif gender == 'F':\n",
        "            return (movie_id, (0, 0, rating, 1)) # 0 Nam, Diem Nu, 1 luot Nu\n",
        "        else:\n",
        "            return (movie_id, (0, 0, 0, 0))\n",
        "\n",
        "    # ReduceByKey: Cong don cac chi so\n",
        "    reduced_stats = movie_gender_rdd.map(map_gender_stats).reduceByKey(\n",
        "        lambda a, b: (a[0]+b[0], a[1]+b[1], a[2]+b[2], a[3]+b[3])\n",
        "    )\n",
        "\n",
        "    # Tinh trung binh: (MovieID, (AvgMale, AvgFemale))\n",
        "    def calculate_averages(record):\n",
        "        movie_id, (sum_m, cnt_m, sum_f, cnt_f) = record\n",
        "\n",
        "        avg_m = sum_m / cnt_m if cnt_m > 0 else 0.0\n",
        "        avg_f = sum_f / cnt_f if cnt_f > 0 else 0.0\n",
        "\n",
        "        return (movie_id, (avg_m, avg_f))\n",
        "\n",
        "    final_stats = reduced_stats.map(calculate_averages)\n",
        "\n",
        "    # Parse Movies: (MovieID, Title)\n",
        "    def parse_movie(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            return (parts[0], parts[1]) # (MovieID, Title)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    movies_parsed = movies_rdd.map(parse_movie).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    # Join: (MovieID, Title) JOIN (MovieID, (AvgMale, AvgFemale)) => (MovieID, (Title, (AvgMale, AvgFemale)))\n",
        "    final_result = movies_parsed.join(final_stats)\n",
        "\n",
        "    def format_output(record):\n",
        "        # record: (MovieID, (Title, (AvgMale, AvgFemale)))\n",
        "        movie_id, (title, (avg_m, avg_f)) = record\n",
        "        return f\"{title} - Male_Avg: {avg_m:.2f}, Female_Avg: {avg_f:.2f}\"\n",
        "    output_lines = final_result.map(format_output)\n",
        "\n",
        "    for line in output_lines.collect():\n",
        "        print(line)\n",
        "\n",
        "# Chay\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"Bai3\").getOrCreate()\n",
        "    bai3()\n",
        "    spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZkg9FcYJB8w",
        "outputId": "c7b0c41f-58e7-496e-8364-14af72c2decc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gladiator (2000) - Male_Avg: 3.59, Female_Avg: 3.64\n",
            "Sunset Boulevard (1950) - Male_Avg: 4.33, Female_Avg: 4.50\n",
            "E.T. the Extra-Terrestrial (1982) - Male_Avg: 3.81, Female_Avg: 3.55\n",
            "The Social Network (2010) - Male_Avg: 4.00, Female_Avg: 3.67\n",
            "Mad Max: Fury Road (2015) - Male_Avg: 4.00, Female_Avg: 3.32\n",
            "The Terminator (1984) - Male_Avg: 3.93, Female_Avg: 4.14\n",
            "Fight Club (1999) - Male_Avg: 3.50, Female_Avg: 3.50\n",
            "Psycho (1960) - Male_Avg: 0.00, Female_Avg: 4.00\n",
            "The Godfather: Part II (1974) - Male_Avg: 4.06, Female_Avg: 3.94\n",
            "The Lord of the Rings: The Fellowship of the Ring (2001) - Male_Avg: 4.00, Female_Avg: 3.80\n",
            "The Lord of the Rings: The Return of the King (2003) - Male_Avg: 3.75, Female_Avg: 3.90\n",
            "The Silence of the Lambs (1991) - Male_Avg: 3.33, Female_Avg: 3.00\n",
            "Lawrence of Arabia (1962) - Male_Avg: 3.55, Female_Avg: 3.31\n",
            "No Country for Old Men (2007) - Male_Avg: 3.92, Female_Avg: 3.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 4:"
      ],
      "metadata": {
        "id": "sNmRMhQ0Qreh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def bai4():\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    try:\n",
        "        ratings_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_1.txt\").union(sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_2.txt\"))\n",
        "        users_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/users.txt\")\n",
        "        movies_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/movies.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"LOI: Khong tim thay file input. {e}\")\n",
        "        return\n",
        "\n",
        "    def get_age_group(age):\n",
        "        try:\n",
        "            age = int(age)\n",
        "            if age <= 18:\n",
        "                return \"0-18\"\n",
        "            elif age <= 35:\n",
        "                return \"18-35\"\n",
        "            elif age <= 50:\n",
        "                return \"35-50\"\n",
        "            else:\n",
        "                return \"50+\"\n",
        "        except:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def parse_user(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            user_id = parts[0]\n",
        "            age = parts[2]\n",
        "            age_group = get_age_group(age)\n",
        "            return (user_id, age_group)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    def parse_rating(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            user_id = parts[0]\n",
        "            movie_id = parts[1]\n",
        "            rating = float(parts[2])\n",
        "            return (user_id, (movie_id, rating))\n",
        "        except:\n",
        "            return (\"ERROR\", (0, 0))\n",
        "\n",
        "    users_parsed = users_rdd.map(parse_user).filter(lambda x: x[0] != \"ERROR\")\n",
        "    ratings_parsed = ratings_rdd.map(parse_rating).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    joined_rdd = ratings_parsed.join(users_parsed)\n",
        "\n",
        "    def map_to_movie_age(record):\n",
        "        user_id, ((movie_id, rating), age_group) = record\n",
        "        return (movie_id, (rating, age_group))\n",
        "\n",
        "    def map_age_stats(record):\n",
        "        movie_id, (rating, group) = record\n",
        "        # (sum_0-18, cnt_0-18, sum_18-35, cnt_18-35, sum_35-50, cnt_35-50, sum_50+, cnt_50+)\n",
        "        stats = [0.0, 0, 0.0, 0, 0.0, 0, 0.0, 0]\n",
        "\n",
        "        if group == \"0-18\":\n",
        "            stats[0] = rating\n",
        "            stats[1] = 1\n",
        "        elif group == \"18-35\":\n",
        "            stats[2] = rating\n",
        "            stats[3] = 1\n",
        "        elif group == \"35-50\":\n",
        "            stats[4] = rating\n",
        "            stats[5] = 1\n",
        "        elif group == \"50+\":\n",
        "            stats[6] = rating\n",
        "            stats[7] = 1\n",
        "\n",
        "        return (movie_id, tuple(stats))\n",
        "\n",
        "    reduced_stats = joined_rdd.map(map_to_movie_age)\\\n",
        "                              .map(map_age_stats)\\\n",
        "                              .reduceByKey(lambda a, b: tuple(x + y for x, y in zip(a, b)))\n",
        "\n",
        "    def calculate_averages(record):\n",
        "        movie_id, stats = record\n",
        "\n",
        "        def safe_avg(total, count):\n",
        "            return total / count if count > 0 else 0.0\n",
        "\n",
        "        avg_0_18 = safe_avg(stats[0], stats[1])\n",
        "        avg_18_35 = safe_avg(stats[2], stats[3])\n",
        "        avg_35_50 = safe_avg(stats[4], stats[5])\n",
        "        avg_50_plus = safe_avg(stats[6], stats[7])\n",
        "\n",
        "        return (movie_id, (avg_0_18, avg_18_35, avg_35_50, avg_50_plus))\n",
        "\n",
        "    final_stats = reduced_stats.map(calculate_averages)\n",
        "\n",
        "    movies_parsed = movies_rdd.map(lambda x: x.split(',')).map(lambda x: (x[0], x[1]))\n",
        "    final_result = movies_parsed.join(final_stats)\n",
        "\n",
        "\n",
        "    # Ham ho tro: Neu gia tri > 0 thi in so, nguoc lai in \"NA\"\n",
        "    def format_val(val):\n",
        "        return f\"{val:.2f}\" if val > 0 else \"NA\"\n",
        "\n",
        "    def format_output(record):\n",
        "        movie_id, (title, (a1, a2, a3, a4)) = record\n",
        "\n",
        "        # Su dung ham format_val da viet o tren\n",
        "        return f\"{title} - [0-18: {format_val(a1)}, 18-35: {format_val(a2)}, 35-50: {format_val(a3)}, 50+: {format_val(a4)}]\"\n",
        "    output_lines = final_result.map(format_output)\n",
        "\n",
        "    for line in output_lines.collect():\n",
        "        print(line)\n",
        "\n",
        "# Chay\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"Bai4\").getOrCreate()\n",
        "    bai4()\n",
        "    spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVEWeywQQueZ",
        "outputId": "b4277644-6752-4c53-f2e9-c6e476377a20"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gladiator (2000) - [0-18: NA, 18-35: 3.44, 35-50: 3.81, 50+: 3.50]\n",
            "Sunset Boulevard (1950) - [0-18: NA, 18-35: 4.17, 35-50: 4.50, 50+: NA]\n",
            "E.T. the Extra-Terrestrial (1982) - [0-18: NA, 18-35: 3.56, 35-50: 3.83, 50+: 3.00]\n",
            "The Social Network (2010) - [0-18: NA, 18-35: 4.00, 35-50: 3.67, 50+: NA]\n",
            "Mad Max: Fury Road (2015) - [0-18: NA, 18-35: 3.36, 35-50: 3.64, 50+: NA]\n",
            "The Terminator (1984) - [0-18: NA, 18-35: 4.17, 35-50: 4.05, 50+: 3.75]\n",
            "Fight Club (1999) - [0-18: NA, 18-35: 3.50, 35-50: 3.50, 50+: 3.50]\n",
            "Psycho (1960) - [0-18: NA, 18-35: 4.50, 35-50: 3.50, 50+: NA]\n",
            "The Godfather: Part II (1974) - [0-18: NA, 18-35: 3.78, 35-50: 4.25, 50+: NA]\n",
            "The Lord of the Rings: The Fellowship of the Ring (2001) - [0-18: NA, 18-35: 4.00, 35-50: 3.83, 50+: NA]\n",
            "The Lord of the Rings: The Return of the King (2003) - [0-18: NA, 18-35: 3.83, 35-50: 3.81, 50+: NA]\n",
            "The Silence of the Lambs (1991) - [0-18: NA, 18-35: 3.00, 35-50: 3.25, 50+: NA]\n",
            "Lawrence of Arabia (1962) - [0-18: NA, 18-35: 3.60, 35-50: 3.29, 50+: 4.50]\n",
            "No Country for Old Men (2007) - [0-18: NA, 18-35: 3.81, 35-50: 3.94, 50+: 4.00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 5:"
      ],
      "metadata": {
        "id": "ZigplnaoRNIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def bai5():\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    try:\n",
        "        # Load ratings (gop 2 file)\n",
        "        ratings_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_1.txt\").union(sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_2.txt\"))\n",
        "        users_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/users.txt\")\n",
        "        occupation_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/occupation.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"LOI: Khong tim thay file. {e}\")\n",
        "        return\n",
        "\n",
        "    # Parse Users: Lay (UserID, OccupationID)\n",
        "    def parse_user(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            user_id = parts[0]\n",
        "            occ_id = parts[3] # Cot OccupationID\n",
        "            return (user_id, occ_id)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    # Parse Ratings: Lay (UserID, Rating)\n",
        "    def parse_rating(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            user_id = parts[0]\n",
        "            rating = float(parts[2])\n",
        "            return (user_id, rating)\n",
        "        except:\n",
        "            return (\"ERROR\", 0.0)\n",
        "\n",
        "    # Parse Occupation: Lay (OccupationID, OccupationName)\n",
        "    def parse_occupation(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            occ_id = parts[0]\n",
        "            occ_name = parts[1]\n",
        "            return (occ_id, occ_name)\n",
        "        except:\n",
        "            return (\"ERROR\", \"ERROR\")\n",
        "\n",
        "    users_parsed = users_rdd.map(parse_user).filter(lambda x: x[0] != \"ERROR\")\n",
        "    ratings_parsed = ratings_rdd.map(parse_rating).filter(lambda x: x[0] != \"ERROR\")\n",
        "    occ_parsed = occupation_rdd.map(parse_occupation).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    # (UserID, Rating) JOIN (UserID, OccID) => (UserID, (Rating, OccID))\n",
        "    joined_user_rating = ratings_parsed.join(users_parsed)\n",
        "\n",
        "    # Chuyen doi sang dang: (OccID, (Rating, 1))\n",
        "    def map_to_occ(record):\n",
        "        user_id, (rating, occ_id) = record\n",
        "        return (occ_id, (rating, 1))\n",
        "\n",
        "    # ReduceByKey: Cong tong diem va tong so luot danh gia cho moi OccID\n",
        "    # (OccID, (SumRating, TotalCount))\n",
        "    occ_stats = joined_user_rating.map(map_to_occ)\\\n",
        "                                  .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "    # (OccID, (SumRating, TotalCount)) JOIN (OccID, OccName) => (OccID, ((SumRating, TotalCount), OccName))\n",
        "    final_data = occ_stats.join(occ_parsed)\n",
        "\n",
        "    def format_output(record):\n",
        "        occ_id, ((total_score, count), occ_name) = record\n",
        "\n",
        "        # Tinh trung binh\n",
        "        avg_rating = total_score / count if count > 0 else 0.0\n",
        "\n",
        "        # Format theo yeu cau\n",
        "        return f\"{occ_name} - TotalRatings: {count}, AverageRating: {avg_rating:.2f}\"\n",
        "\n",
        "    # Sort theo ten nghe nghiep\n",
        "    output_lines = final_data.map(format_output).sortBy(lambda x: x)\n",
        "\n",
        "    for line in output_lines.collect():\n",
        "        print(line)\n",
        "\n",
        "# Chay\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"Bai5\").getOrCreate()\n",
        "    bai5()\n",
        "    spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3h_J5s1RvzJ",
        "outputId": "2f34bccb-19b8-43c4-dc3e-64731731d677"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accountant - TotalRatings: 6, AverageRating: 3.58\n",
            "Artist - TotalRatings: 11, AverageRating: 3.73\n",
            "Consultant - TotalRatings: 14, AverageRating: 3.86\n",
            "Designer - TotalRatings: 13, AverageRating: 4.00\n",
            "Doctor - TotalRatings: 21, AverageRating: 3.69\n",
            "Engineer - TotalRatings: 18, AverageRating: 3.56\n",
            "Journalist - TotalRatings: 17, AverageRating: 3.85\n",
            "Lawyer - TotalRatings: 17, AverageRating: 3.65\n",
            "Manager - TotalRatings: 16, AverageRating: 3.47\n",
            "Nurse - TotalRatings: 11, AverageRating: 3.86\n",
            "Programmer - TotalRatings: 10, AverageRating: 4.25\n",
            "Salesperson - TotalRatings: 17, AverageRating: 3.65\n",
            "Student - TotalRatings: 8, AverageRating: 4.00\n",
            "Teacher - TotalRatings: 5, AverageRating: 3.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 6"
      ],
      "metadata": {
        "id": "S8gtdwxFRxDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from datetime import datetime\n",
        "\n",
        "def bai6():\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    try:\n",
        "        # Load ratings (gop 2 file)\n",
        "        ratings_rdd = sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_1.txt\").union(sc.textFile(\"/content/drive/My Drive/BigData/BTTH2/ratings_2.txt\"))\n",
        "    except Exception as e:\n",
        "        print(f\"LOI: Khong tim thay file. {e}\")\n",
        "        return\n",
        "\n",
        "    def parse_rating_year(line):\n",
        "        try:\n",
        "            parts = line.split(',')\n",
        "            rating = float(parts[2])\n",
        "            timestamp = int(parts[3])\n",
        "\n",
        "            # Chuyen doi Timestamp (Unix epoch) sang Nam (YYYY)\n",
        "            dt_object = datetime.fromtimestamp(timestamp)\n",
        "            year = dt_object.year\n",
        "\n",
        "            return (year, (rating, 1)) # (Nam, (Diem, 1_luot))\n",
        "        except:\n",
        "            return (\"ERROR\", (0, 0))\n",
        "\n",
        "    # Map va Filter loi\n",
        "    year_ratings = ratings_rdd.map(parse_rating_year).filter(lambda x: x[0] != \"ERROR\")\n",
        "\n",
        "    # ReduceByKey: Cong tong diem va so luot danh gia theo tung nam\n",
        "    # (Year, (SumRating, Count))\n",
        "    reduced_stats = year_ratings.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "    # Tinh trung binh: (Year, (TotalRatings, AvgRating))\n",
        "    def calculate_stats(record):\n",
        "        year, (total_score, count) = record\n",
        "        avg_rating = total_score / count if count > 0 else 0.0\n",
        "        return (year, (count, avg_rating))\n",
        "\n",
        "    final_stats = reduced_stats.map(calculate_stats)\n",
        "\n",
        "    # Sap xep theo Nam tang dan (sortByKey)\n",
        "    sorted_stats = final_stats.sortByKey()\n",
        "\n",
        "    def format_output(record):\n",
        "        year, (count, avg) = record\n",
        "        # Format theo yeu cau: \"Year - TotalRatings: xx, AverageRating: xx\"\n",
        "        return f\"{year} - TotalRatings: {count}, AverageRating: {avg:.2f}\"\n",
        "\n",
        "    output_lines = sorted_stats.map(format_output)\n",
        "\n",
        "    # Collect va in ket qua\n",
        "    for line in output_lines.collect():\n",
        "        print(line)\n",
        "\n",
        "# Chay\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder.appName(\"Bai6\").getOrCreate()\n",
        "    bai6()\n",
        "    spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7e6XuJBSFvh",
        "outputId": "9b4c7fd5-56d3-4f4b-cd86-7cd32db282aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020 - TotalRatings: 184, AverageRating: 3.75\n"
          ]
        }
      ]
    }
  ]
}